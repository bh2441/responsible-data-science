# Bias Audit of Toxic Comment Classifier

This repository contains a final group project completed for DS-UA 202: Responsible Data Science at New York University in Spring 2025. In this project, my teammate Daphne Ozkan and I conducted a fairness audit of a multi-label text classification model trained on the Jigsaw Unintended Bias in Toxicity Classification dataset. The classifier was built to detect various types of toxic online content, including toxic, severe toxic, obscene, threat, insult, and identity-based hate comments. Our primary goal was to evaluate trade-offs between model accuracy and fairness, especially regarding identity terms (e.g., “I am Muslim”). We experimented with TF-IDF and logistic regression, applied SMOTE to address class imbalance, and used LIME, PCA, and t-SNE for interpretability. Results showed that while SMOTE significantly improved recall for rare toxic categories (up to 75%), it also increased false positives, underscoring the tension between sensitivity and precision. LIME helped us better understand which features drove model decisions, revealing patterns of bias. Our audit concluded with actionable insights and recommendations for improving fairness in content moderation systems.
